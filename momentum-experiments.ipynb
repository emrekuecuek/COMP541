{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet\n",
    "using LinearAlgebra\n",
    "using AutoGrad: full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct SM3\n",
    "    lr::AbstractFloat\n",
    "    eps::AbstractFloat\n",
    "    gclip::AbstractFloat\n",
    "    momentum::AbstractFloat\n",
    "    velocity\n",
    "    dims\n",
    "    accumulators\n",
    "end\n",
    "\n",
    "for T in (Array{Float32},Array{Float64},KnetArray{Float32},KnetArray{Float64}); @eval begin\n",
    "    function Knet.update!(w::$T, g, p::SM3)\n",
    "        Knet.gclip!(g, p.gclip)\n",
    "        g = full(g)\n",
    "        if p.accumulators==nothing; \n",
    "            p.dims=size(w);\n",
    "            p.accumulators=[KnetArray(zeros(Float32, _shape_for_broadcasting(p.dims, i))) for i in 1:length(p.dims)];\n",
    "        end\n",
    "        accumulator = _compute_past_accumulator(p.accumulators, p.dims)\n",
    "        accumulator .+= g.*g\n",
    "#         #TODO: Add momentum tensor for scaled gradient\n",
    "# #         momentum_tensor = KnetArray(Float32(p.momentum) .+ zeros(Float32, size(w)))\n",
    "#         if p.velocity == nothing; p.velocity = zero(w); end\n",
    "#         lmul!(p.momentum, p.velocity);\n",
    "        \n",
    "#         scaled_g = (1.0 .- p.velocity) .* (g ./(sqrt.(accumulator .+ p.eps)))\n",
    "\n",
    "#         if 0 < p.momentum\n",
    "#             p.velocity += p.velocity .* (p.velocity .- 1.0) .+ scaled_g\n",
    "#             update = p.velocity\n",
    "#         else\n",
    "#             update = scaled_g            \n",
    "#         end\n",
    "#         axpy!(p.lr, update, w)\n",
    "        axpy!(-p.lr, g./(sqrt.(accumulator .+ p.eps)), w)\n",
    "        p.accumulators = _accumulator_updater(p.accumulators, p.dims, accumulator)\n",
    "        \n",
    "    end\n",
    "end;end\n",
    "\n",
    "SM3(; lr=0.001, eps=1e-30, gclip=0.0, momentum=0.9) = SM3(lr, eps, gclip, momentum, nothing, nothing, nothing)\n",
    "sm3(f,d; lr=0.001, eps=1e-30, gclip=0.0, momentum=0.9, o...) = Knet.minimize(f,d,SM3(lr, eps, gclip, momentum, nothing, nothing, nothing);o...)\n",
    "sm3!(x...;o...) = for y in sm3(x...;o...); end\n",
    "Knet.clone(a::SM3) = SM3(a.lr, a.eps, a.gclip, a.momentum, nothing, nothing, nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_accumulator_updater (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function _shape_for_broadcasting(dims, desired)\n",
    "    rank = length(dims)\n",
    "    return tuple([i==desired ? dims[i] : 1 for i in 1:rank]...)\n",
    "end\n",
    "\n",
    "function _compute_past_accumulator(accumulators, dims)\n",
    "    rank = length(dims)\n",
    "    accumulators_for_broadcasting = [\n",
    "        reshape(accumulators[i], _shape_for_broadcasting(dims, i))\n",
    "        for i in 1:rank]\n",
    "    \n",
    "    result = accumulators_for_broadcasting[1]\n",
    "#     return result\n",
    "    # Check if min is doing for number of elmns.\n",
    "    for i in 1:rank\n",
    "        result = min.(result, accumulators_for_broadcasting[i])\n",
    "    end\n",
    "    return result\n",
    "    \n",
    "end\n",
    "\n",
    "function _accumulator_updater(accumulators, dims, update_tensor)\n",
    "    rank = length(dims)\n",
    "    for i in 1:rank\n",
    "        max_dims = []\n",
    "#       max_dims = [i!=j ? j : for j in 1:rank]\n",
    "#       TODO: Make this by array comprehension.\n",
    "        for j in 1:rank\n",
    "            if i!=j\n",
    "                append!(max_dims, j)\n",
    "            end\n",
    "        end\n",
    "        accumulators[i] = max.(accumulators[i], maximum(update_tensor, dims=tuple(max_dims...)))\n",
    "    end\n",
    "    return accumulators\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.953"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Knet\n",
    "\n",
    "# Define convolutional layer:\n",
    "struct Conv; w; b; f; end\n",
    "(c::Conv)(x) = c.f.(pool(conv4(c.w, x) .+ c.b))\n",
    "Conv(w1,w2,cx,cy,f=relu) = Conv(param(w1,w2,cx,cy), param0(1,1,cy,1), f)\n",
    "\n",
    "# Define dense layer:\n",
    "struct Dense; w; b; f; end\n",
    "(d::Dense)(x) = d.f.(d.w * mat(x) .+ d.b)\n",
    "Dense(i::Int,o::Int,f=relu) = Dense(param(o,i), param0(o), f)\n",
    "\n",
    "# Define a chain of layers and a loss function:\n",
    "struct Chain; layers; end\n",
    "(c::Chain)(x) = (for l in c.layers; x = l(x); end; x)\n",
    "(c::Chain)(x,y) = nll(c(x),y)\n",
    "\n",
    "# Load MNIST data:\n",
    "include(Knet.dir(\"data\",\"mnist.jl\"))\n",
    "dtrn, dtst = mnistdata(batchsize=100)\n",
    "\n",
    "# Define, train and test LeNet (about 30 secs on a gpu to reach 99% accuracy)\n",
    "LeNet = Chain((Conv(5,5,1,20), Conv(5,5,20,50), Dense(800,500), Dense(500,10,identity)))\n",
    "sm3!(LeNet, repeat(dtrn,10))\n",
    "accuracy(LeNet, dtst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
