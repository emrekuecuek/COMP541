{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KnetArray{Float64, 3}(undef, (2,3,4));\n",
    "# a = [KnetArray{Float64,3}(undef,(10, 1, 1)),\n",
    "#     KnetArray{Float64,3}(undef,(1, 784, 1)),\n",
    "#     KnetArray{Float64,3}(undef,(1, 1, 100))\n",
    "#     ];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet, LinearAlgebra, AutoGrad\n",
    "using AutoGrad: full\n",
    "using Random\n",
    "Random.seed!(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct SM3\n",
    "    lr#::AbstractFloat\n",
    "    eps#::AbstractFloat\n",
    "    gclip#::AbstractFloat\n",
    "    dims\n",
    "#     momentum::AbstractArray{AbstractFloat}\n",
    "    accumulators#::AbstractArray{AbstractFloat}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SM3(; lr=0.001, eps=1e-30, gclip=0.0) = SM3(lr, eps, gclip, nothing, nothing)\n",
    "sm3(f,d; lr=0.001, eps=1e-30, gclip=0.0,o...) = Knet.minimize(f,d,SM3(lr, eps, gclip, nothing, nothing))\n",
    "sm3!(x...;o...) = for y in sm3(x...;o...); end\n",
    "Knet.clone(a::SM3) = SM3(a.lr, a.eps, a.gclip, nothing, nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for T in (Array{Float32},Array{Float64},KnetArray{Float32},KnetArray{Float64}); @eval begin\n",
    "    function Knet.update!(w::$T, g, p::SM3)\n",
    "        Knet.gclip!(g, p.gclip)\n",
    "        g = full(g)\n",
    "        if p.accumulators==nothing; \n",
    "            p.dims=size(w);\n",
    "            p.accumulators=[KnetArray(zeros(Float32, _shape_for_broadcasting(p.dims, i))) for i in 1:length(p.dims)];\n",
    "        end\n",
    "        accumulator = _compute_past_accumulator(p.accumulators, p.dims)\n",
    "        accumulator .+= g.*g\n",
    "        #TODO: Add momentum tensor for scaled gradient\n",
    "        axpy!(-p.lr, g./(sqrt.(accumulator .+ p.eps)), w)\n",
    "        #TODO: Add accumulator updates\n",
    "        p.accumulators = _accumulator_updater(p.accumulators, p.dims, accumulator)\n",
    "    end\n",
    "end;end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_shape_for_broadcasting (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function _shape_for_broadcasting(dims, desired)\n",
    "    rank = length(dims)\n",
    "    return tuple([i==desired ? dims[i] : 1 for i in 1:rank]...)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_compute_past_accumulator (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function _compute_past_accumulator(accumulators, dims)\n",
    "    rank = length(dims)\n",
    "    accumulators_for_broadcasting = [\n",
    "        reshape(accumulators[i], _shape_for_broadcasting(dims, i))\n",
    "        for i in 1:rank]\n",
    "    \n",
    "    result = accumulators_for_broadcasting[1]\n",
    "#     return result\n",
    "    # Check if min is doing for number of elmns.\n",
    "    for i in 1:rank\n",
    "        result = min.(result, accumulators_for_broadcasting[i])\n",
    "    end\n",
    "    return result\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_accumulator_updater (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function _accumulator_updater(accumulators, dims, update_tensor)\n",
    "    rank = length(dims)\n",
    "    for i in 1:rank\n",
    "        max_dims = []\n",
    "#       max_dims = [i!=j ? j : for j in 1:rank]\n",
    "#       TODO: Make this by array comprehension.\n",
    "        for j in 1:rank\n",
    "            if i!=j\n",
    "                append!(max_dims, j)\n",
    "            end\n",
    "        end\n",
    "        accumulators[i] = max.(accumulators[i], maximum(update_tensor, dims=tuple(max_dims...)))\n",
    "    end\n",
    "    return accumulators\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [KnetArray{Float64,3}(undef,(10, 1, 1)),\n",
    "#     KnetArray{Float64,3}(undef,(1, 784, 1)),\n",
    "#     KnetArray{Float64,3}(undef,(1, 1, 100))\n",
    "#     ];\n",
    "# b = [10, 784, 100];\n",
    "# c = rand(Float64, (10,784,100));\n",
    "# _compute_past_accumulator(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading MNIST...\n",
      "└ @ Main /kuacc/users/ekucuk19/.julia/packages/Knet/bTNMd/data/mnist.jl:33\n",
      "┌ Warning: repeat(d::Data,n) is deprecated, use IterTools.ncycle instead.\n",
      "└ @ Knet /kuacc/users/ekucuk19/.julia/packages/Knet/bTNMd/src/data.jl:92\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9549"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define convolutional layer:\n",
    "struct Conv; w; b; f; end\n",
    "(c::Conv)(x) = c.f.(pool(conv4(c.w, x) .+ c.b))\n",
    "Conv(w1,w2,cx,cy,f=relu) = Conv(param(w1,w2,cx,cy), param0(1,1,cy,1), f)\n",
    "\n",
    "# Define dense layer:\n",
    "struct Dense; w; b; f; end\n",
    "(d::Dense)(x) = d.f.(d.w * mat(x) .+ d.b)\n",
    "Dense(i::Int,o::Int,f=relu) = Dense(param(o,i), param0(o), f)\n",
    "\n",
    "# Define a chain of layers and a loss function:\n",
    "struct Chain; layers; end\n",
    "(c::Chain)(x) = (for l in c.layers; x = l(x); end; x)\n",
    "(c::Chain)(x,y) = nll(c(x),y)\n",
    "\n",
    "# Load MNIST data:\n",
    "include(Knet.dir(\"data\",\"mnist.jl\"))\n",
    "dtrn, dtst = mnistdata()\n",
    "\n",
    "# Define, train and test LeNet (about 30 secs on a gpu to reach 99% accuracy)\n",
    "LeNet = Chain((Conv(5,5,1,20), Conv(5,5,20,50), Dense(800,500), Dense(500,10,identity)))\n",
    "sm3!(LeNet, repeat(dtrn,10))\n",
    "accuracy(LeNet, dtst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
